{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold, GroupShuffleSplit, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "df_bots_tweets = pd.read_csv('./content_polluters_tweets.txt',\n",
    "                            sep='\\t',\n",
    "                            header=None,\n",
    "                            names=[\"UserID\", \"TweetID\", \"Tweet\", \"CreatedAt\"])\n",
    "\n",
    "df_humans_tweets = pd.read_csv('./legitimate_users_tweets.txt',\n",
    "                              sep='\\t',\n",
    "                              header=None,\n",
    "                              names=[\"UserID\", \"TweetID\", \"Tweet\", \"CreatedAt\"])\n",
    "\n",
    "df_bots_tweets['is_bot'] = 1\n",
    "df_humans_tweets['is_bot'] = 0\n",
    "\n",
    "df_bots_tweets = df_bots_tweets.dropna(subset=['Tweet'])\n",
    "df_humans_tweets = df_humans_tweets.dropna(subset=['Tweet'])\n",
    "\n",
    "df_bots_tweets = df_bots_tweets.iloc[:25_000]\n",
    "df_humans_tweets = df_humans_tweets.iloc[:25_000]\n",
    "\n",
    "df_combined = pd.concat([df_bots_tweets, df_humans_tweets], axis=0, ignore_index=True)\n",
    "\n",
    "df_combined = df_combined.dropna(subset=['Tweet'])\n",
    "df_combined['Tweet'] = df_combined['Tweet'].astype(str)\n",
    "df_combined = df_combined[df_combined['Tweet'].str.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_humans_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bots_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_combined['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>TweetID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>CreatedAt</th>\n",
       "      <th>is_bot</th>\n",
       "      <th>Tweet Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6301</td>\n",
       "      <td>5599519501</td>\n",
       "      <td>MELBOURNE ENQUIRY: Seeking a variety of acts f...</td>\n",
       "      <td>2009-11-10 15:14:31</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.14196883, -0.20637092, 0.53259736, -0.00723...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6301</td>\n",
       "      <td>5600313663</td>\n",
       "      <td>THE BURLESQUE BOOTCAMP SYDNEY - Open Date tick...</td>\n",
       "      <td>2009-11-10 15:46:05</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.2789788, -0.2811198, 0.6468443, 0.18883522,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6301</td>\n",
       "      <td>5600328557</td>\n",
       "      <td>THE BURLESQUE BOOTCAMP SYDNEY - Open Date tick...</td>\n",
       "      <td>2009-11-10 15:46:40</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.28744933, -0.25366488, 0.67137194, 0.101002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6301</td>\n",
       "      <td>5600338093</td>\n",
       "      <td>THE BURLESQUE BOOTCAMP SYDNEY - Open Date tick...</td>\n",
       "      <td>2009-11-10 15:47:03</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.21801575, -0.22536404, 0.63394326, 0.078535...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6301</td>\n",
       "      <td>5600564863</td>\n",
       "      <td>Come to \"The Burlesque Bootcamp - Sydney\" Satu...</td>\n",
       "      <td>2009-11-10 15:56:03</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.15251878, -0.28992698, 0.8231413, -0.060256...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID     TweetID                                              Tweet  \\\n",
       "0    6301  5599519501  MELBOURNE ENQUIRY: Seeking a variety of acts f...   \n",
       "1    6301  5600313663  THE BURLESQUE BOOTCAMP SYDNEY - Open Date tick...   \n",
       "2    6301  5600328557  THE BURLESQUE BOOTCAMP SYDNEY - Open Date tick...   \n",
       "3    6301  5600338093  THE BURLESQUE BOOTCAMP SYDNEY - Open Date tick...   \n",
       "4    6301  5600564863  Come to \"The Burlesque Bootcamp - Sydney\" Satu...   \n",
       "\n",
       "             CreatedAt  is_bot  \\\n",
       "0  2009-11-10 15:14:31       1   \n",
       "1  2009-11-10 15:46:05       1   \n",
       "2  2009-11-10 15:46:40       1   \n",
       "3  2009-11-10 15:47:03       1   \n",
       "4  2009-11-10 15:56:03       1   \n",
       "\n",
       "                                     Tweet Embedding  \n",
       "0  [0.14196883, -0.20637092, 0.53259736, -0.00723...  \n",
       "1  [0.2789788, -0.2811198, 0.6468443, 0.18883522,...  \n",
       "2  [0.28744933, -0.25366488, 0.67137194, 0.101002...  \n",
       "3  [0.21801575, -0.22536404, 0.63394326, 0.078535...  \n",
       "4  [0.15251878, -0.28992698, 0.8231413, -0.060256...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['Tweet Embedding'] = df_combined['Tweet'].apply(get_bert_embedding)\n",
    "df_combined.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_combined['Tweet Embedding']\n",
    "y = df_combined['is_bot']\n",
    "groups = df_combined['UserID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97722/4284739793.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  X_tensor = torch.tensor(X.tolist(), dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "X_tensor = torch.tensor(X.tolist(), dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(X))\n",
    "test_size = len(X) - train_size\n",
    "train_dataset, test_dataset = random_split(TensorDataset(X_tensor, y_tensor), [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2048, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class BotPredictionNN(nn.Module):\n",
    "  def __init__(self, input_dim):\n",
    "      super(BotPredictionNN, self).__init__()\n",
    "      self.fc = nn.Sequential(\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(input_dim, 256),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(256, 1),\n",
    "        nn.Sigmoid()\n",
    "      )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "input_dim = X_tensor.shape[1]\n",
    "model = BotPredictionNN(input_dim).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.LinearLR(optimizer, total_iters=20)\n",
    "# scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Training Loss: 0.6917, Test Loss: 0.6886, Best Test Loss: 0.6886\n",
      "Epoch [2/1000], Training Loss: 0.6854, Test Loss: 0.6813, Best Test Loss: 0.6813\n",
      "Epoch [3/1000], Training Loss: 0.6757, Test Loss: 0.6692, Best Test Loss: 0.6692\n",
      "Epoch [4/1000], Training Loss: 0.6599, Test Loss: 0.6499, Best Test Loss: 0.6499\n",
      "Epoch [5/1000], Training Loss: 0.6380, Test Loss: 0.6271, Best Test Loss: 0.6271\n",
      "Epoch [6/1000], Training Loss: 0.6181, Test Loss: 0.6108, Best Test Loss: 0.6108\n",
      "Epoch [7/1000], Training Loss: 0.6062, Test Loss: 0.6020, Best Test Loss: 0.6020\n",
      "Epoch [8/1000], Training Loss: 0.5992, Test Loss: 0.5943, Best Test Loss: 0.5943\n",
      "Epoch [9/1000], Training Loss: 0.5922, Test Loss: 0.5874, Best Test Loss: 0.5874\n",
      "Epoch [10/1000], Training Loss: 0.5872, Test Loss: 0.5809, Best Test Loss: 0.5809\n",
      "Epoch [11/1000], Training Loss: 0.5822, Test Loss: 0.5747, Best Test Loss: 0.5747\n",
      "Epoch [12/1000], Training Loss: 0.5774, Test Loss: 0.5694, Best Test Loss: 0.5694\n",
      "Epoch [13/1000], Training Loss: 0.5735, Test Loss: 0.5648, Best Test Loss: 0.5648\n",
      "Epoch [14/1000], Training Loss: 0.5719, Test Loss: 0.5626, Best Test Loss: 0.5626\n",
      "Epoch [15/1000], Training Loss: 0.5702, Test Loss: 0.5586, Best Test Loss: 0.5586\n",
      "Epoch [16/1000], Training Loss: 0.5667, Test Loss: 0.5564, Best Test Loss: 0.5564\n",
      "Epoch [17/1000], Training Loss: 0.5651, Test Loss: 0.5542, Best Test Loss: 0.5542\n",
      "Epoch [18/1000], Training Loss: 0.5637, Test Loss: 0.5523, Best Test Loss: 0.5523\n",
      "Epoch [19/1000], Training Loss: 0.5605, Test Loss: 0.5508, Best Test Loss: 0.5508\n",
      "Epoch [20/1000], Training Loss: 0.5601, Test Loss: 0.5495, Best Test Loss: 0.5495\n",
      "Epoch [21/1000], Training Loss: 0.5601, Test Loss: 0.5479, Best Test Loss: 0.5479\n",
      "Epoch [22/1000], Training Loss: 0.5586, Test Loss: 0.5469, Best Test Loss: 0.5469\n",
      "Epoch [23/1000], Training Loss: 0.5578, Test Loss: 0.5455, Best Test Loss: 0.5455\n",
      "Epoch [24/1000], Training Loss: 0.5563, Test Loss: 0.5456, Best Test Loss: 0.5455\n",
      "Epoch [25/1000], Training Loss: 0.5570, Test Loss: 0.5439, Best Test Loss: 0.5439\n",
      "Epoch [26/1000], Training Loss: 0.5545, Test Loss: 0.5432, Best Test Loss: 0.5432\n",
      "Epoch [27/1000], Training Loss: 0.5545, Test Loss: 0.5419, Best Test Loss: 0.5419\n",
      "Epoch [28/1000], Training Loss: 0.5535, Test Loss: 0.5410, Best Test Loss: 0.5410\n",
      "Epoch [29/1000], Training Loss: 0.5524, Test Loss: 0.5400, Best Test Loss: 0.5400\n",
      "Epoch [30/1000], Training Loss: 0.5507, Test Loss: 0.5398, Best Test Loss: 0.5398\n",
      "Epoch [31/1000], Training Loss: 0.5505, Test Loss: 0.5394, Best Test Loss: 0.5394\n",
      "Epoch [32/1000], Training Loss: 0.5498, Test Loss: 0.5380, Best Test Loss: 0.5380\n",
      "Epoch [33/1000], Training Loss: 0.5488, Test Loss: 0.5375, Best Test Loss: 0.5375\n",
      "Epoch [34/1000], Training Loss: 0.5492, Test Loss: 0.5369, Best Test Loss: 0.5369\n",
      "Epoch [35/1000], Training Loss: 0.5480, Test Loss: 0.5365, Best Test Loss: 0.5365\n",
      "Epoch [36/1000], Training Loss: 0.5459, Test Loss: 0.5359, Best Test Loss: 0.5359\n",
      "Epoch [37/1000], Training Loss: 0.5466, Test Loss: 0.5346, Best Test Loss: 0.5346\n",
      "Epoch [38/1000], Training Loss: 0.5468, Test Loss: 0.5346, Best Test Loss: 0.5346\n",
      "Epoch [39/1000], Training Loss: 0.5437, Test Loss: 0.5333, Best Test Loss: 0.5333\n",
      "Epoch [40/1000], Training Loss: 0.5452, Test Loss: 0.5328, Best Test Loss: 0.5328\n",
      "Epoch [41/1000], Training Loss: 0.5451, Test Loss: 0.5324, Best Test Loss: 0.5324\n",
      "Epoch [42/1000], Training Loss: 0.5459, Test Loss: 0.5324, Best Test Loss: 0.5324\n",
      "Epoch [43/1000], Training Loss: 0.5446, Test Loss: 0.5317, Best Test Loss: 0.5317\n",
      "Epoch [44/1000], Training Loss: 0.5438, Test Loss: 0.5311, Best Test Loss: 0.5311\n",
      "Epoch [45/1000], Training Loss: 0.5437, Test Loss: 0.5308, Best Test Loss: 0.5308\n",
      "Epoch [46/1000], Training Loss: 0.5440, Test Loss: 0.5303, Best Test Loss: 0.5303\n",
      "Epoch [47/1000], Training Loss: 0.5437, Test Loss: 0.5298, Best Test Loss: 0.5298\n",
      "Epoch [48/1000], Training Loss: 0.5435, Test Loss: 0.5294, Best Test Loss: 0.5294\n",
      "Epoch [49/1000], Training Loss: 0.5405, Test Loss: 0.5285, Best Test Loss: 0.5285\n",
      "Epoch [50/1000], Training Loss: 0.5425, Test Loss: 0.5280, Best Test Loss: 0.5280\n",
      "Epoch [51/1000], Training Loss: 0.5391, Test Loss: 0.5273, Best Test Loss: 0.5273\n",
      "Epoch [52/1000], Training Loss: 0.5390, Test Loss: 0.5266, Best Test Loss: 0.5266\n",
      "Epoch [53/1000], Training Loss: 0.5401, Test Loss: 0.5267, Best Test Loss: 0.5266\n",
      "Epoch [54/1000], Training Loss: 0.5381, Test Loss: 0.5261, Best Test Loss: 0.5261\n",
      "Epoch [55/1000], Training Loss: 0.5378, Test Loss: 0.5253, Best Test Loss: 0.5253\n",
      "Epoch [56/1000], Training Loss: 0.5393, Test Loss: 0.5252, Best Test Loss: 0.5252\n",
      "Epoch [57/1000], Training Loss: 0.5391, Test Loss: 0.5246, Best Test Loss: 0.5246\n",
      "Epoch [58/1000], Training Loss: 0.5369, Test Loss: 0.5241, Best Test Loss: 0.5241\n",
      "Epoch [59/1000], Training Loss: 0.5372, Test Loss: 0.5236, Best Test Loss: 0.5236\n",
      "Epoch [60/1000], Training Loss: 0.5347, Test Loss: 0.5232, Best Test Loss: 0.5232\n",
      "Epoch [61/1000], Training Loss: 0.5359, Test Loss: 0.5227, Best Test Loss: 0.5227\n",
      "Epoch [62/1000], Training Loss: 0.5346, Test Loss: 0.5219, Best Test Loss: 0.5219\n",
      "Epoch [63/1000], Training Loss: 0.5336, Test Loss: 0.5214, Best Test Loss: 0.5214\n",
      "Epoch [64/1000], Training Loss: 0.5334, Test Loss: 0.5209, Best Test Loss: 0.5209\n",
      "Epoch [65/1000], Training Loss: 0.5351, Test Loss: 0.5213, Best Test Loss: 0.5209\n",
      "Epoch [66/1000], Training Loss: 0.5334, Test Loss: 0.5203, Best Test Loss: 0.5203\n",
      "Epoch [67/1000], Training Loss: 0.5336, Test Loss: 0.5204, Best Test Loss: 0.5203\n",
      "Epoch [68/1000], Training Loss: 0.5313, Test Loss: 0.5197, Best Test Loss: 0.5197\n",
      "Epoch [69/1000], Training Loss: 0.5313, Test Loss: 0.5190, Best Test Loss: 0.5190\n",
      "Epoch [70/1000], Training Loss: 0.5335, Test Loss: 0.5187, Best Test Loss: 0.5187\n",
      "Epoch [71/1000], Training Loss: 0.5318, Test Loss: 0.5187, Best Test Loss: 0.5187\n",
      "Epoch [72/1000], Training Loss: 0.5316, Test Loss: 0.5178, Best Test Loss: 0.5178\n",
      "Epoch [73/1000], Training Loss: 0.5299, Test Loss: 0.5177, Best Test Loss: 0.5177\n",
      "Epoch [74/1000], Training Loss: 0.5313, Test Loss: 0.5175, Best Test Loss: 0.5175\n",
      "Epoch [75/1000], Training Loss: 0.5298, Test Loss: 0.5172, Best Test Loss: 0.5172\n",
      "Epoch [76/1000], Training Loss: 0.5286, Test Loss: 0.5162, Best Test Loss: 0.5162\n",
      "Epoch [77/1000], Training Loss: 0.5290, Test Loss: 0.5158, Best Test Loss: 0.5158\n",
      "Epoch [78/1000], Training Loss: 0.5287, Test Loss: 0.5162, Best Test Loss: 0.5158\n",
      "Epoch [79/1000], Training Loss: 0.5281, Test Loss: 0.5152, Best Test Loss: 0.5152\n",
      "Epoch [80/1000], Training Loss: 0.5287, Test Loss: 0.5150, Best Test Loss: 0.5150\n",
      "Epoch [81/1000], Training Loss: 0.5288, Test Loss: 0.5151, Best Test Loss: 0.5150\n",
      "Epoch [82/1000], Training Loss: 0.5274, Test Loss: 0.5145, Best Test Loss: 0.5145\n",
      "Epoch [83/1000], Training Loss: 0.5276, Test Loss: 0.5135, Best Test Loss: 0.5135\n",
      "Epoch [84/1000], Training Loss: 0.5264, Test Loss: 0.5131, Best Test Loss: 0.5131\n",
      "Epoch [85/1000], Training Loss: 0.5262, Test Loss: 0.5138, Best Test Loss: 0.5131\n",
      "Epoch [86/1000], Training Loss: 0.5252, Test Loss: 0.5124, Best Test Loss: 0.5124\n",
      "Epoch [87/1000], Training Loss: 0.5263, Test Loss: 0.5125, Best Test Loss: 0.5124\n",
      "Epoch [88/1000], Training Loss: 0.5241, Test Loss: 0.5116, Best Test Loss: 0.5116\n",
      "Epoch [89/1000], Training Loss: 0.5254, Test Loss: 0.5118, Best Test Loss: 0.5116\n",
      "Epoch [90/1000], Training Loss: 0.5229, Test Loss: 0.5106, Best Test Loss: 0.5106\n",
      "Epoch [91/1000], Training Loss: 0.5245, Test Loss: 0.5106, Best Test Loss: 0.5106\n",
      "Epoch [92/1000], Training Loss: 0.5239, Test Loss: 0.5104, Best Test Loss: 0.5104\n",
      "Epoch [93/1000], Training Loss: 0.5247, Test Loss: 0.5104, Best Test Loss: 0.5104\n",
      "Epoch [94/1000], Training Loss: 0.5221, Test Loss: 0.5102, Best Test Loss: 0.5102\n",
      "Epoch [95/1000], Training Loss: 0.5220, Test Loss: 0.5099, Best Test Loss: 0.5099\n",
      "Epoch [96/1000], Training Loss: 0.5226, Test Loss: 0.5091, Best Test Loss: 0.5091\n",
      "Epoch [97/1000], Training Loss: 0.5192, Test Loss: 0.5088, Best Test Loss: 0.5088\n",
      "Epoch [98/1000], Training Loss: 0.5225, Test Loss: 0.5088, Best Test Loss: 0.5088\n",
      "Epoch [99/1000], Training Loss: 0.5200, Test Loss: 0.5094, Best Test Loss: 0.5088\n",
      "Epoch [100/1000], Training Loss: 0.5213, Test Loss: 0.5077, Best Test Loss: 0.5077\n",
      "Epoch [101/1000], Training Loss: 0.5199, Test Loss: 0.5073, Best Test Loss: 0.5073\n",
      "Epoch [102/1000], Training Loss: 0.5186, Test Loss: 0.5067, Best Test Loss: 0.5067\n",
      "Epoch [103/1000], Training Loss: 0.5210, Test Loss: 0.5072, Best Test Loss: 0.5067\n",
      "Epoch [104/1000], Training Loss: 0.5206, Test Loss: 0.5080, Best Test Loss: 0.5067\n",
      "Epoch [105/1000], Training Loss: 0.5192, Test Loss: 0.5065, Best Test Loss: 0.5065\n",
      "Epoch [106/1000], Training Loss: 0.5179, Test Loss: 0.5053, Best Test Loss: 0.5053\n",
      "Epoch [107/1000], Training Loss: 0.5185, Test Loss: 0.5071, Best Test Loss: 0.5053\n",
      "Epoch [108/1000], Training Loss: 0.5190, Test Loss: 0.5052, Best Test Loss: 0.5052\n",
      "Epoch [109/1000], Training Loss: 0.5181, Test Loss: 0.5049, Best Test Loss: 0.5049\n",
      "Epoch [110/1000], Training Loss: 0.5161, Test Loss: 0.5043, Best Test Loss: 0.5043\n",
      "Epoch [111/1000], Training Loss: 0.5160, Test Loss: 0.5043, Best Test Loss: 0.5043\n",
      "Epoch [112/1000], Training Loss: 0.5168, Test Loss: 0.5039, Best Test Loss: 0.5039\n",
      "Epoch [113/1000], Training Loss: 0.5165, Test Loss: 0.5038, Best Test Loss: 0.5038\n",
      "Epoch [114/1000], Training Loss: 0.5167, Test Loss: 0.5036, Best Test Loss: 0.5036\n",
      "Epoch [115/1000], Training Loss: 0.5151, Test Loss: 0.5029, Best Test Loss: 0.5029\n",
      "Epoch [116/1000], Training Loss: 0.5166, Test Loss: 0.5025, Best Test Loss: 0.5025\n",
      "Epoch [117/1000], Training Loss: 0.5141, Test Loss: 0.5026, Best Test Loss: 0.5025\n",
      "Epoch [118/1000], Training Loss: 0.5159, Test Loss: 0.5021, Best Test Loss: 0.5021\n",
      "Epoch [119/1000], Training Loss: 0.5130, Test Loss: 0.5016, Best Test Loss: 0.5016\n",
      "Epoch [120/1000], Training Loss: 0.5147, Test Loss: 0.5020, Best Test Loss: 0.5016\n",
      "Epoch [121/1000], Training Loss: 0.5139, Test Loss: 0.5012, Best Test Loss: 0.5012\n",
      "Epoch [122/1000], Training Loss: 0.5138, Test Loss: 0.5013, Best Test Loss: 0.5012\n",
      "Epoch [123/1000], Training Loss: 0.5142, Test Loss: 0.5008, Best Test Loss: 0.5008\n",
      "Epoch [124/1000], Training Loss: 0.5146, Test Loss: 0.5015, Best Test Loss: 0.5008\n",
      "Epoch [125/1000], Training Loss: 0.5121, Test Loss: 0.5010, Best Test Loss: 0.5008\n",
      "Epoch [126/1000], Training Loss: 0.5145, Test Loss: 0.5005, Best Test Loss: 0.5005\n",
      "Epoch [127/1000], Training Loss: 0.5109, Test Loss: 0.5001, Best Test Loss: 0.5001\n",
      "Epoch [128/1000], Training Loss: 0.5117, Test Loss: 0.4994, Best Test Loss: 0.4994\n",
      "Epoch [129/1000], Training Loss: 0.5104, Test Loss: 0.4993, Best Test Loss: 0.4993\n",
      "Epoch [130/1000], Training Loss: 0.5122, Test Loss: 0.4991, Best Test Loss: 0.4991\n",
      "Epoch [131/1000], Training Loss: 0.5117, Test Loss: 0.4994, Best Test Loss: 0.4991\n",
      "Epoch [132/1000], Training Loss: 0.5117, Test Loss: 0.4989, Best Test Loss: 0.4989\n",
      "Epoch [133/1000], Training Loss: 0.5125, Test Loss: 0.4995, Best Test Loss: 0.4989\n",
      "Epoch [134/1000], Training Loss: 0.5104, Test Loss: 0.4985, Best Test Loss: 0.4985\n",
      "Epoch [135/1000], Training Loss: 0.5105, Test Loss: 0.4974, Best Test Loss: 0.4974\n",
      "Epoch [136/1000], Training Loss: 0.5099, Test Loss: 0.4972, Best Test Loss: 0.4972\n",
      "Epoch [137/1000], Training Loss: 0.5121, Test Loss: 0.4973, Best Test Loss: 0.4972\n",
      "Epoch [138/1000], Training Loss: 0.5097, Test Loss: 0.4974, Best Test Loss: 0.4972\n",
      "Epoch [139/1000], Training Loss: 0.5079, Test Loss: 0.4969, Best Test Loss: 0.4969\n",
      "Epoch [140/1000], Training Loss: 0.5085, Test Loss: 0.4963, Best Test Loss: 0.4963\n",
      "Epoch [141/1000], Training Loss: 0.5071, Test Loss: 0.4961, Best Test Loss: 0.4961\n",
      "Epoch [142/1000], Training Loss: 0.5071, Test Loss: 0.4957, Best Test Loss: 0.4957\n",
      "Epoch [143/1000], Training Loss: 0.5072, Test Loss: 0.4954, Best Test Loss: 0.4954\n",
      "Epoch [144/1000], Training Loss: 0.5072, Test Loss: 0.4948, Best Test Loss: 0.4948\n",
      "Epoch [145/1000], Training Loss: 0.5086, Test Loss: 0.4954, Best Test Loss: 0.4948\n",
      "Epoch [146/1000], Training Loss: 0.5063, Test Loss: 0.4946, Best Test Loss: 0.4946\n",
      "Epoch [147/1000], Training Loss: 0.5070, Test Loss: 0.4956, Best Test Loss: 0.4946\n",
      "Epoch [148/1000], Training Loss: 0.5072, Test Loss: 0.4946, Best Test Loss: 0.4946\n",
      "Epoch [149/1000], Training Loss: 0.5070, Test Loss: 0.4940, Best Test Loss: 0.4940\n",
      "Epoch [150/1000], Training Loss: 0.5056, Test Loss: 0.4950, Best Test Loss: 0.4940\n",
      "Epoch [151/1000], Training Loss: 0.5074, Test Loss: 0.4939, Best Test Loss: 0.4939\n",
      "Epoch [152/1000], Training Loss: 0.5083, Test Loss: 0.4939, Best Test Loss: 0.4939\n",
      "Epoch [153/1000], Training Loss: 0.5050, Test Loss: 0.4933, Best Test Loss: 0.4933\n",
      "Epoch [154/1000], Training Loss: 0.5039, Test Loss: 0.4930, Best Test Loss: 0.4930\n",
      "Epoch [155/1000], Training Loss: 0.5061, Test Loss: 0.4925, Best Test Loss: 0.4925\n",
      "Epoch [156/1000], Training Loss: 0.5048, Test Loss: 0.4928, Best Test Loss: 0.4925\n",
      "Epoch [157/1000], Training Loss: 0.5047, Test Loss: 0.4925, Best Test Loss: 0.4925\n",
      "Epoch [158/1000], Training Loss: 0.5028, Test Loss: 0.4921, Best Test Loss: 0.4921\n",
      "Epoch [159/1000], Training Loss: 0.5038, Test Loss: 0.4919, Best Test Loss: 0.4919\n",
      "Epoch [160/1000], Training Loss: 0.5016, Test Loss: 0.4909, Best Test Loss: 0.4909\n",
      "Epoch [161/1000], Training Loss: 0.5065, Test Loss: 0.4918, Best Test Loss: 0.4909\n",
      "Epoch [162/1000], Training Loss: 0.5042, Test Loss: 0.4922, Best Test Loss: 0.4909\n",
      "Epoch [163/1000], Training Loss: 0.5026, Test Loss: 0.4910, Best Test Loss: 0.4909\n",
      "Epoch [164/1000], Training Loss: 0.5003, Test Loss: 0.4909, Best Test Loss: 0.4909\n",
      "Epoch [165/1000], Training Loss: 0.5033, Test Loss: 0.4906, Best Test Loss: 0.4906\n",
      "Epoch [166/1000], Training Loss: 0.5047, Test Loss: 0.4906, Best Test Loss: 0.4906\n",
      "Epoch [167/1000], Training Loss: 0.4992, Test Loss: 0.4899, Best Test Loss: 0.4899\n",
      "Epoch [168/1000], Training Loss: 0.4999, Test Loss: 0.4902, Best Test Loss: 0.4899\n",
      "Epoch [169/1000], Training Loss: 0.5026, Test Loss: 0.4907, Best Test Loss: 0.4899\n",
      "Epoch [170/1000], Training Loss: 0.5017, Test Loss: 0.4900, Best Test Loss: 0.4899\n",
      "Epoch [171/1000], Training Loss: 0.5005, Test Loss: 0.4895, Best Test Loss: 0.4895\n",
      "Epoch [172/1000], Training Loss: 0.4998, Test Loss: 0.4891, Best Test Loss: 0.4891\n",
      "Epoch [173/1000], Training Loss: 0.5003, Test Loss: 0.4886, Best Test Loss: 0.4886\n",
      "Epoch [174/1000], Training Loss: 0.4993, Test Loss: 0.4883, Best Test Loss: 0.4883\n",
      "Epoch [175/1000], Training Loss: 0.4994, Test Loss: 0.4878, Best Test Loss: 0.4878\n",
      "Epoch [176/1000], Training Loss: 0.4989, Test Loss: 0.4878, Best Test Loss: 0.4878\n",
      "Epoch [177/1000], Training Loss: 0.4996, Test Loss: 0.4879, Best Test Loss: 0.4878\n",
      "Epoch [178/1000], Training Loss: 0.4980, Test Loss: 0.4874, Best Test Loss: 0.4874\n",
      "Epoch [179/1000], Training Loss: 0.4998, Test Loss: 0.4880, Best Test Loss: 0.4874\n",
      "Epoch [180/1000], Training Loss: 0.4982, Test Loss: 0.4871, Best Test Loss: 0.4871\n",
      "Epoch [181/1000], Training Loss: 0.4994, Test Loss: 0.4872, Best Test Loss: 0.4871\n",
      "Epoch [182/1000], Training Loss: 0.4977, Test Loss: 0.4875, Best Test Loss: 0.4871\n",
      "Epoch [183/1000], Training Loss: 0.4959, Test Loss: 0.4865, Best Test Loss: 0.4865\n",
      "Epoch [184/1000], Training Loss: 0.4964, Test Loss: 0.4868, Best Test Loss: 0.4865\n",
      "Epoch [185/1000], Training Loss: 0.4958, Test Loss: 0.4857, Best Test Loss: 0.4857\n",
      "Epoch [186/1000], Training Loss: 0.4958, Test Loss: 0.4861, Best Test Loss: 0.4857\n",
      "Epoch [187/1000], Training Loss: 0.4962, Test Loss: 0.4866, Best Test Loss: 0.4857\n",
      "Epoch [188/1000], Training Loss: 0.4960, Test Loss: 0.4852, Best Test Loss: 0.4852\n",
      "Epoch [189/1000], Training Loss: 0.4967, Test Loss: 0.4854, Best Test Loss: 0.4852\n",
      "Epoch [190/1000], Training Loss: 0.4966, Test Loss: 0.4862, Best Test Loss: 0.4852\n",
      "Epoch [191/1000], Training Loss: 0.4959, Test Loss: 0.4844, Best Test Loss: 0.4844\n",
      "Epoch [192/1000], Training Loss: 0.4942, Test Loss: 0.4842, Best Test Loss: 0.4842\n",
      "Epoch [193/1000], Training Loss: 0.4967, Test Loss: 0.4855, Best Test Loss: 0.4842\n",
      "Epoch [194/1000], Training Loss: 0.4947, Test Loss: 0.4854, Best Test Loss: 0.4842\n",
      "Epoch [195/1000], Training Loss: 0.4964, Test Loss: 0.4842, Best Test Loss: 0.4842\n",
      "Epoch [196/1000], Training Loss: 0.4954, Test Loss: 0.4846, Best Test Loss: 0.4842\n",
      "Epoch [197/1000], Training Loss: 0.4951, Test Loss: 0.4842, Best Test Loss: 0.4842\n",
      "Epoch [198/1000], Training Loss: 0.4943, Test Loss: 0.4836, Best Test Loss: 0.4836\n",
      "Epoch [199/1000], Training Loss: 0.4952, Test Loss: 0.4838, Best Test Loss: 0.4836\n",
      "Epoch [200/1000], Training Loss: 0.4925, Test Loss: 0.4841, Best Test Loss: 0.4836\n",
      "Epoch [201/1000], Training Loss: 0.4925, Test Loss: 0.4835, Best Test Loss: 0.4835\n",
      "Epoch [202/1000], Training Loss: 0.4930, Test Loss: 0.4837, Best Test Loss: 0.4835\n",
      "Epoch [203/1000], Training Loss: 0.4936, Test Loss: 0.4829, Best Test Loss: 0.4829\n",
      "Epoch [204/1000], Training Loss: 0.4938, Test Loss: 0.4829, Best Test Loss: 0.4829\n",
      "Epoch [205/1000], Training Loss: 0.4918, Test Loss: 0.4820, Best Test Loss: 0.4820\n",
      "Epoch [206/1000], Training Loss: 0.4924, Test Loss: 0.4819, Best Test Loss: 0.4819\n",
      "Epoch [207/1000], Training Loss: 0.4927, Test Loss: 0.4821, Best Test Loss: 0.4819\n",
      "Epoch [208/1000], Training Loss: 0.4939, Test Loss: 0.4840, Best Test Loss: 0.4819\n",
      "Epoch [209/1000], Training Loss: 0.4930, Test Loss: 0.4822, Best Test Loss: 0.4819\n",
      "Epoch [210/1000], Training Loss: 0.4890, Test Loss: 0.4811, Best Test Loss: 0.4811\n",
      "Epoch [211/1000], Training Loss: 0.4906, Test Loss: 0.4806, Best Test Loss: 0.4806\n",
      "Epoch [212/1000], Training Loss: 0.4955, Test Loss: 0.4812, Best Test Loss: 0.4806\n",
      "Epoch [213/1000], Training Loss: 0.4894, Test Loss: 0.4810, Best Test Loss: 0.4806\n",
      "Epoch [214/1000], Training Loss: 0.4919, Test Loss: 0.4805, Best Test Loss: 0.4805\n",
      "Epoch [215/1000], Training Loss: 0.4926, Test Loss: 0.4807, Best Test Loss: 0.4805\n",
      "Epoch [216/1000], Training Loss: 0.4894, Test Loss: 0.4808, Best Test Loss: 0.4805\n",
      "Epoch [217/1000], Training Loss: 0.4897, Test Loss: 0.4799, Best Test Loss: 0.4799\n",
      "Epoch [218/1000], Training Loss: 0.4896, Test Loss: 0.4798, Best Test Loss: 0.4798\n",
      "Epoch [219/1000], Training Loss: 0.4891, Test Loss: 0.4803, Best Test Loss: 0.4798\n",
      "Epoch [220/1000], Training Loss: 0.4894, Test Loss: 0.4804, Best Test Loss: 0.4798\n",
      "Epoch [221/1000], Training Loss: 0.4900, Test Loss: 0.4801, Best Test Loss: 0.4798\n",
      "Epoch [222/1000], Training Loss: 0.4888, Test Loss: 0.4790, Best Test Loss: 0.4790\n",
      "Epoch [223/1000], Training Loss: 0.4899, Test Loss: 0.4792, Best Test Loss: 0.4790\n",
      "Epoch [224/1000], Training Loss: 0.4893, Test Loss: 0.4813, Best Test Loss: 0.4790\n",
      "Epoch [225/1000], Training Loss: 0.4881, Test Loss: 0.4795, Best Test Loss: 0.4790\n",
      "Epoch [226/1000], Training Loss: 0.4857, Test Loss: 0.4780, Best Test Loss: 0.4780\n",
      "Epoch [227/1000], Training Loss: 0.4889, Test Loss: 0.4784, Best Test Loss: 0.4780\n",
      "Epoch [228/1000], Training Loss: 0.4890, Test Loss: 0.4790, Best Test Loss: 0.4780\n",
      "Epoch [229/1000], Training Loss: 0.4881, Test Loss: 0.4785, Best Test Loss: 0.4780\n",
      "Epoch [230/1000], Training Loss: 0.4879, Test Loss: 0.4791, Best Test Loss: 0.4780\n",
      "Epoch [231/1000], Training Loss: 0.4882, Test Loss: 0.4780, Best Test Loss: 0.4780\n",
      "Epoch [232/1000], Training Loss: 0.4856, Test Loss: 0.4778, Best Test Loss: 0.4778\n",
      "Epoch [233/1000], Training Loss: 0.4847, Test Loss: 0.4778, Best Test Loss: 0.4778\n",
      "Epoch [234/1000], Training Loss: 0.4876, Test Loss: 0.4774, Best Test Loss: 0.4774\n",
      "Epoch [235/1000], Training Loss: 0.4893, Test Loss: 0.4780, Best Test Loss: 0.4774\n",
      "Epoch [236/1000], Training Loss: 0.4847, Test Loss: 0.4795, Best Test Loss: 0.4774\n",
      "Epoch [237/1000], Training Loss: 0.4882, Test Loss: 0.4774, Best Test Loss: 0.4774\n",
      "Epoch [238/1000], Training Loss: 0.4863, Test Loss: 0.4770, Best Test Loss: 0.4770\n",
      "Epoch [239/1000], Training Loss: 0.4856, Test Loss: 0.4767, Best Test Loss: 0.4767\n",
      "Epoch [240/1000], Training Loss: 0.4850, Test Loss: 0.4766, Best Test Loss: 0.4766\n",
      "Epoch [241/1000], Training Loss: 0.4861, Test Loss: 0.4767, Best Test Loss: 0.4766\n",
      "Epoch [242/1000], Training Loss: 0.4849, Test Loss: 0.4761, Best Test Loss: 0.4761\n",
      "Epoch [243/1000], Training Loss: 0.4863, Test Loss: 0.4777, Best Test Loss: 0.4761\n",
      "Epoch [244/1000], Training Loss: 0.4860, Test Loss: 0.4771, Best Test Loss: 0.4761\n",
      "Epoch [245/1000], Training Loss: 0.4858, Test Loss: 0.4757, Best Test Loss: 0.4757\n",
      "Epoch [246/1000], Training Loss: 0.4851, Test Loss: 0.4769, Best Test Loss: 0.4757\n",
      "Epoch [247/1000], Training Loss: 0.4840, Test Loss: 0.4757, Best Test Loss: 0.4757\n",
      "Epoch [248/1000], Training Loss: 0.4841, Test Loss: 0.4763, Best Test Loss: 0.4757\n",
      "Epoch [249/1000], Training Loss: 0.4829, Test Loss: 0.4760, Best Test Loss: 0.4757\n",
      "Epoch [250/1000], Training Loss: 0.4837, Test Loss: 0.4757, Best Test Loss: 0.4757\n",
      "Epoch [251/1000], Training Loss: 0.4826, Test Loss: 0.4770, Best Test Loss: 0.4757\n",
      "Epoch [252/1000], Training Loss: 0.4824, Test Loss: 0.4749, Best Test Loss: 0.4749\n",
      "Epoch [253/1000], Training Loss: 0.4811, Test Loss: 0.4747, Best Test Loss: 0.4747\n",
      "Epoch [254/1000], Training Loss: 0.4827, Test Loss: 0.4758, Best Test Loss: 0.4747\n",
      "Epoch [255/1000], Training Loss: 0.4813, Test Loss: 0.4739, Best Test Loss: 0.4739\n",
      "Epoch [256/1000], Training Loss: 0.4820, Test Loss: 0.4750, Best Test Loss: 0.4739\n",
      "Epoch [257/1000], Training Loss: 0.4830, Test Loss: 0.4777, Best Test Loss: 0.4739\n",
      "Epoch [258/1000], Training Loss: 0.4800, Test Loss: 0.4747, Best Test Loss: 0.4739\n",
      "Epoch [259/1000], Training Loss: 0.4808, Test Loss: 0.4740, Best Test Loss: 0.4739\n",
      "Epoch [260/1000], Training Loss: 0.4814, Test Loss: 0.4736, Best Test Loss: 0.4736\n",
      "Epoch [261/1000], Training Loss: 0.4829, Test Loss: 0.4754, Best Test Loss: 0.4736\n",
      "Epoch [262/1000], Training Loss: 0.4821, Test Loss: 0.4744, Best Test Loss: 0.4736\n",
      "Epoch [263/1000], Training Loss: 0.4814, Test Loss: 0.4741, Best Test Loss: 0.4736\n",
      "Epoch [264/1000], Training Loss: 0.4777, Test Loss: 0.4731, Best Test Loss: 0.4731\n",
      "Epoch [265/1000], Training Loss: 0.4799, Test Loss: 0.4729, Best Test Loss: 0.4729\n",
      "Epoch [266/1000], Training Loss: 0.4803, Test Loss: 0.4728, Best Test Loss: 0.4728\n",
      "Epoch [267/1000], Training Loss: 0.4801, Test Loss: 0.4733, Best Test Loss: 0.4728\n",
      "Epoch [268/1000], Training Loss: 0.4782, Test Loss: 0.4727, Best Test Loss: 0.4727\n",
      "Epoch [269/1000], Training Loss: 0.4809, Test Loss: 0.4745, Best Test Loss: 0.4727\n",
      "Epoch [270/1000], Training Loss: 0.4799, Test Loss: 0.4735, Best Test Loss: 0.4727\n",
      "Epoch [271/1000], Training Loss: 0.4776, Test Loss: 0.4722, Best Test Loss: 0.4722\n",
      "Epoch [272/1000], Training Loss: 0.4783, Test Loss: 0.4714, Best Test Loss: 0.4714\n",
      "Epoch [273/1000], Training Loss: 0.4778, Test Loss: 0.4716, Best Test Loss: 0.4714\n",
      "Epoch [274/1000], Training Loss: 0.4809, Test Loss: 0.4720, Best Test Loss: 0.4714\n",
      "Epoch [275/1000], Training Loss: 0.4781, Test Loss: 0.4732, Best Test Loss: 0.4714\n",
      "Epoch [276/1000], Training Loss: 0.4803, Test Loss: 0.4727, Best Test Loss: 0.4714\n",
      "Epoch [277/1000], Training Loss: 0.4775, Test Loss: 0.4724, Best Test Loss: 0.4714\n",
      "Epoch [278/1000], Training Loss: 0.4771, Test Loss: 0.4733, Best Test Loss: 0.4714\n",
      "Epoch [279/1000], Training Loss: 0.4817, Test Loss: 0.4719, Best Test Loss: 0.4714\n",
      "Epoch [280/1000], Training Loss: 0.4758, Test Loss: 0.4721, Best Test Loss: 0.4714\n",
      "Epoch [281/1000], Training Loss: 0.4780, Test Loss: 0.4706, Best Test Loss: 0.4706\n",
      "Epoch [282/1000], Training Loss: 0.4781, Test Loss: 0.4711, Best Test Loss: 0.4706\n",
      "Epoch [283/1000], Training Loss: 0.4782, Test Loss: 0.4704, Best Test Loss: 0.4704\n",
      "Epoch [284/1000], Training Loss: 0.4770, Test Loss: 0.4709, Best Test Loss: 0.4704\n",
      "Epoch [285/1000], Training Loss: 0.4768, Test Loss: 0.4710, Best Test Loss: 0.4704\n",
      "Epoch [286/1000], Training Loss: 0.4783, Test Loss: 0.4704, Best Test Loss: 0.4704\n",
      "Epoch [287/1000], Training Loss: 0.4747, Test Loss: 0.4704, Best Test Loss: 0.4704\n",
      "Epoch [288/1000], Training Loss: 0.4744, Test Loss: 0.4704, Best Test Loss: 0.4704\n",
      "Epoch [289/1000], Training Loss: 0.4770, Test Loss: 0.4709, Best Test Loss: 0.4704\n",
      "Epoch [290/1000], Training Loss: 0.4750, Test Loss: 0.4692, Best Test Loss: 0.4692\n",
      "Epoch [291/1000], Training Loss: 0.4771, Test Loss: 0.4713, Best Test Loss: 0.4692\n",
      "Epoch [292/1000], Training Loss: 0.4727, Test Loss: 0.4705, Best Test Loss: 0.4692\n",
      "Epoch [293/1000], Training Loss: 0.4734, Test Loss: 0.4690, Best Test Loss: 0.4690\n",
      "Epoch [294/1000], Training Loss: 0.4729, Test Loss: 0.4707, Best Test Loss: 0.4690\n",
      "Epoch [295/1000], Training Loss: 0.4743, Test Loss: 0.4687, Best Test Loss: 0.4687\n",
      "Epoch [296/1000], Training Loss: 0.4749, Test Loss: 0.4700, Best Test Loss: 0.4687\n",
      "Epoch [297/1000], Training Loss: 0.4741, Test Loss: 0.4695, Best Test Loss: 0.4687\n",
      "Epoch [298/1000], Training Loss: 0.4738, Test Loss: 0.4689, Best Test Loss: 0.4687\n",
      "Epoch [299/1000], Training Loss: 0.4740, Test Loss: 0.4683, Best Test Loss: 0.4683\n",
      "Epoch [300/1000], Training Loss: 0.4750, Test Loss: 0.4703, Best Test Loss: 0.4683\n",
      "Epoch [301/1000], Training Loss: 0.4756, Test Loss: 0.4690, Best Test Loss: 0.4683\n",
      "Epoch [302/1000], Training Loss: 0.4753, Test Loss: 0.4688, Best Test Loss: 0.4683\n",
      "Epoch [303/1000], Training Loss: 0.4724, Test Loss: 0.4685, Best Test Loss: 0.4683\n",
      "Epoch [304/1000], Training Loss: 0.4734, Test Loss: 0.4689, Best Test Loss: 0.4683\n",
      "Epoch [305/1000], Training Loss: 0.4725, Test Loss: 0.4681, Best Test Loss: 0.4681\n",
      "Epoch [306/1000], Training Loss: 0.4746, Test Loss: 0.4705, Best Test Loss: 0.4681\n",
      "Epoch [307/1000], Training Loss: 0.4728, Test Loss: 0.4707, Best Test Loss: 0.4681\n",
      "Epoch [308/1000], Training Loss: 0.4729, Test Loss: 0.4678, Best Test Loss: 0.4678\n",
      "Epoch [309/1000], Training Loss: 0.4725, Test Loss: 0.4678, Best Test Loss: 0.4678\n",
      "Epoch [310/1000], Training Loss: 0.4729, Test Loss: 0.4688, Best Test Loss: 0.4678\n",
      "Epoch [311/1000], Training Loss: 0.4684, Test Loss: 0.4671, Best Test Loss: 0.4671\n",
      "Epoch [312/1000], Training Loss: 0.4740, Test Loss: 0.4684, Best Test Loss: 0.4671\n",
      "Epoch [313/1000], Training Loss: 0.4705, Test Loss: 0.4674, Best Test Loss: 0.4671\n",
      "Epoch [314/1000], Training Loss: 0.4687, Test Loss: 0.4679, Best Test Loss: 0.4671\n",
      "Epoch [315/1000], Training Loss: 0.4692, Test Loss: 0.4670, Best Test Loss: 0.4670\n",
      "Epoch [316/1000], Training Loss: 0.4718, Test Loss: 0.4675, Best Test Loss: 0.4670\n",
      "Epoch [317/1000], Training Loss: 0.4692, Test Loss: 0.4665, Best Test Loss: 0.4665\n",
      "Epoch [318/1000], Training Loss: 0.4721, Test Loss: 0.4664, Best Test Loss: 0.4664\n",
      "Epoch [319/1000], Training Loss: 0.4698, Test Loss: 0.4673, Best Test Loss: 0.4664\n",
      "Epoch [320/1000], Training Loss: 0.4710, Test Loss: 0.4666, Best Test Loss: 0.4664\n",
      "Epoch [321/1000], Training Loss: 0.4714, Test Loss: 0.4664, Best Test Loss: 0.4664\n",
      "Epoch [322/1000], Training Loss: 0.4707, Test Loss: 0.4665, Best Test Loss: 0.4664\n",
      "Epoch [323/1000], Training Loss: 0.4724, Test Loss: 0.4668, Best Test Loss: 0.4664\n",
      "Epoch [324/1000], Training Loss: 0.4674, Test Loss: 0.4659, Best Test Loss: 0.4659\n",
      "Epoch [325/1000], Training Loss: 0.4687, Test Loss: 0.4675, Best Test Loss: 0.4659\n",
      "Epoch [326/1000], Training Loss: 0.4697, Test Loss: 0.4657, Best Test Loss: 0.4657\n",
      "Epoch [327/1000], Training Loss: 0.4692, Test Loss: 0.4663, Best Test Loss: 0.4657\n",
      "Epoch [328/1000], Training Loss: 0.4707, Test Loss: 0.4667, Best Test Loss: 0.4657\n",
      "Epoch [329/1000], Training Loss: 0.4678, Test Loss: 0.4650, Best Test Loss: 0.4650\n",
      "Epoch [330/1000], Training Loss: 0.4698, Test Loss: 0.4673, Best Test Loss: 0.4650\n",
      "Epoch [331/1000], Training Loss: 0.4696, Test Loss: 0.4657, Best Test Loss: 0.4650\n",
      "Epoch [332/1000], Training Loss: 0.4672, Test Loss: 0.4642, Best Test Loss: 0.4642\n",
      "Epoch [333/1000], Training Loss: 0.4707, Test Loss: 0.4668, Best Test Loss: 0.4642\n",
      "Epoch [334/1000], Training Loss: 0.4657, Test Loss: 0.4654, Best Test Loss: 0.4642\n",
      "Epoch [335/1000], Training Loss: 0.4674, Test Loss: 0.4647, Best Test Loss: 0.4642\n",
      "Epoch [336/1000], Training Loss: 0.4674, Test Loss: 0.4647, Best Test Loss: 0.4642\n",
      "Epoch [337/1000], Training Loss: 0.4694, Test Loss: 0.4650, Best Test Loss: 0.4642\n",
      "Epoch [338/1000], Training Loss: 0.4682, Test Loss: 0.4664, Best Test Loss: 0.4642\n",
      "Epoch [339/1000], Training Loss: 0.4657, Test Loss: 0.4642, Best Test Loss: 0.4642\n",
      "Epoch [340/1000], Training Loss: 0.4663, Test Loss: 0.4662, Best Test Loss: 0.4642\n",
      "Epoch [341/1000], Training Loss: 0.4670, Test Loss: 0.4645, Best Test Loss: 0.4642\n",
      "Epoch [342/1000], Training Loss: 0.4675, Test Loss: 0.4638, Best Test Loss: 0.4638\n",
      "Epoch [343/1000], Training Loss: 0.4676, Test Loss: 0.4643, Best Test Loss: 0.4638\n",
      "Epoch [344/1000], Training Loss: 0.4693, Test Loss: 0.4656, Best Test Loss: 0.4638\n",
      "Epoch [345/1000], Training Loss: 0.4661, Test Loss: 0.4644, Best Test Loss: 0.4638\n",
      "Epoch [346/1000], Training Loss: 0.4647, Test Loss: 0.4643, Best Test Loss: 0.4638\n",
      "Epoch [347/1000], Training Loss: 0.4675, Test Loss: 0.4636, Best Test Loss: 0.4636\n",
      "Epoch [348/1000], Training Loss: 0.4642, Test Loss: 0.4633, Best Test Loss: 0.4633\n",
      "Epoch [349/1000], Training Loss: 0.4681, Test Loss: 0.4639, Best Test Loss: 0.4633\n",
      "Epoch [350/1000], Training Loss: 0.4671, Test Loss: 0.4648, Best Test Loss: 0.4633\n",
      "Epoch [351/1000], Training Loss: 0.4631, Test Loss: 0.4632, Best Test Loss: 0.4632\n",
      "Epoch [352/1000], Training Loss: 0.4637, Test Loss: 0.4629, Best Test Loss: 0.4629\n",
      "Epoch [353/1000], Training Loss: 0.4682, Test Loss: 0.4633, Best Test Loss: 0.4629\n",
      "Epoch [354/1000], Training Loss: 0.4667, Test Loss: 0.4638, Best Test Loss: 0.4629\n",
      "Epoch [355/1000], Training Loss: 0.4647, Test Loss: 0.4633, Best Test Loss: 0.4629\n",
      "Epoch [356/1000], Training Loss: 0.4648, Test Loss: 0.4632, Best Test Loss: 0.4629\n",
      "Epoch [357/1000], Training Loss: 0.4653, Test Loss: 0.4625, Best Test Loss: 0.4625\n",
      "Epoch [358/1000], Training Loss: 0.4654, Test Loss: 0.4632, Best Test Loss: 0.4625\n",
      "Epoch [359/1000], Training Loss: 0.4664, Test Loss: 0.4631, Best Test Loss: 0.4625\n",
      "Epoch [360/1000], Training Loss: 0.4636, Test Loss: 0.4636, Best Test Loss: 0.4625\n",
      "Epoch [361/1000], Training Loss: 0.4637, Test Loss: 0.4628, Best Test Loss: 0.4625\n",
      "Epoch [362/1000], Training Loss: 0.4629, Test Loss: 0.4630, Best Test Loss: 0.4625\n",
      "Epoch [363/1000], Training Loss: 0.4640, Test Loss: 0.4631, Best Test Loss: 0.4625\n",
      "Epoch [364/1000], Training Loss: 0.4610, Test Loss: 0.4621, Best Test Loss: 0.4621\n",
      "Epoch [365/1000], Training Loss: 0.4628, Test Loss: 0.4633, Best Test Loss: 0.4621\n",
      "Epoch [366/1000], Training Loss: 0.4601, Test Loss: 0.4627, Best Test Loss: 0.4621\n",
      "Epoch [367/1000], Training Loss: 0.4642, Test Loss: 0.4630, Best Test Loss: 0.4621\n",
      "Epoch [368/1000], Training Loss: 0.4651, Test Loss: 0.4619, Best Test Loss: 0.4619\n",
      "Epoch [369/1000], Training Loss: 0.4629, Test Loss: 0.4618, Best Test Loss: 0.4618\n",
      "Epoch [370/1000], Training Loss: 0.4642, Test Loss: 0.4627, Best Test Loss: 0.4618\n",
      "Epoch [371/1000], Training Loss: 0.4633, Test Loss: 0.4617, Best Test Loss: 0.4617\n",
      "Epoch [372/1000], Training Loss: 0.4604, Test Loss: 0.4620, Best Test Loss: 0.4617\n",
      "Epoch [373/1000], Training Loss: 0.4615, Test Loss: 0.4621, Best Test Loss: 0.4617\n",
      "Epoch [374/1000], Training Loss: 0.4633, Test Loss: 0.4612, Best Test Loss: 0.4612\n",
      "Epoch [375/1000], Training Loss: 0.4622, Test Loss: 0.4608, Best Test Loss: 0.4608\n",
      "Epoch [376/1000], Training Loss: 0.4626, Test Loss: 0.4629, Best Test Loss: 0.4608\n",
      "Epoch [377/1000], Training Loss: 0.4628, Test Loss: 0.4619, Best Test Loss: 0.4608\n",
      "Epoch [378/1000], Training Loss: 0.4621, Test Loss: 0.4607, Best Test Loss: 0.4607\n",
      "Epoch [379/1000], Training Loss: 0.4615, Test Loss: 0.4616, Best Test Loss: 0.4607\n",
      "Epoch [380/1000], Training Loss: 0.4593, Test Loss: 0.4616, Best Test Loss: 0.4607\n",
      "Epoch [381/1000], Training Loss: 0.4605, Test Loss: 0.4617, Best Test Loss: 0.4607\n",
      "Epoch [382/1000], Training Loss: 0.4605, Test Loss: 0.4613, Best Test Loss: 0.4607\n",
      "Epoch [383/1000], Training Loss: 0.4598, Test Loss: 0.4610, Best Test Loss: 0.4607\n",
      "Epoch [384/1000], Training Loss: 0.4637, Test Loss: 0.4616, Best Test Loss: 0.4607\n",
      "Epoch [385/1000], Training Loss: 0.4630, Test Loss: 0.4605, Best Test Loss: 0.4605\n",
      "Epoch [386/1000], Training Loss: 0.4619, Test Loss: 0.4614, Best Test Loss: 0.4605\n",
      "Epoch [387/1000], Training Loss: 0.4621, Test Loss: 0.4620, Best Test Loss: 0.4605\n",
      "Epoch [388/1000], Training Loss: 0.4616, Test Loss: 0.4609, Best Test Loss: 0.4605\n",
      "Epoch [389/1000], Training Loss: 0.4610, Test Loss: 0.4600, Best Test Loss: 0.4600\n",
      "Epoch [390/1000], Training Loss: 0.4603, Test Loss: 0.4606, Best Test Loss: 0.4600\n",
      "Epoch [391/1000], Training Loss: 0.4610, Test Loss: 0.4605, Best Test Loss: 0.4600\n",
      "Epoch [392/1000], Training Loss: 0.4585, Test Loss: 0.4616, Best Test Loss: 0.4600\n",
      "Epoch [393/1000], Training Loss: 0.4599, Test Loss: 0.4597, Best Test Loss: 0.4597\n",
      "Epoch [394/1000], Training Loss: 0.4573, Test Loss: 0.4597, Best Test Loss: 0.4597\n",
      "Epoch [395/1000], Training Loss: 0.4595, Test Loss: 0.4593, Best Test Loss: 0.4593\n",
      "Epoch [396/1000], Training Loss: 0.4589, Test Loss: 0.4606, Best Test Loss: 0.4593\n",
      "Epoch [397/1000], Training Loss: 0.4587, Test Loss: 0.4629, Best Test Loss: 0.4593\n",
      "Epoch [398/1000], Training Loss: 0.4601, Test Loss: 0.4600, Best Test Loss: 0.4593\n",
      "Epoch [399/1000], Training Loss: 0.4590, Test Loss: 0.4599, Best Test Loss: 0.4593\n",
      "Epoch [400/1000], Training Loss: 0.4591, Test Loss: 0.4612, Best Test Loss: 0.4593\n",
      "Epoch [401/1000], Training Loss: 0.4584, Test Loss: 0.4590, Best Test Loss: 0.4590\n",
      "Epoch [402/1000], Training Loss: 0.4567, Test Loss: 0.4601, Best Test Loss: 0.4590\n",
      "Epoch [403/1000], Training Loss: 0.4575, Test Loss: 0.4604, Best Test Loss: 0.4590\n",
      "Epoch [404/1000], Training Loss: 0.4576, Test Loss: 0.4620, Best Test Loss: 0.4590\n",
      "Epoch [405/1000], Training Loss: 0.4598, Test Loss: 0.4588, Best Test Loss: 0.4588\n",
      "Epoch [406/1000], Training Loss: 0.4562, Test Loss: 0.4589, Best Test Loss: 0.4588\n",
      "Epoch [407/1000], Training Loss: 0.4605, Test Loss: 0.4588, Best Test Loss: 0.4588\n",
      "Epoch [408/1000], Training Loss: 0.4579, Test Loss: 0.4604, Best Test Loss: 0.4588\n",
      "Epoch [409/1000], Training Loss: 0.4603, Test Loss: 0.4594, Best Test Loss: 0.4588\n",
      "Epoch [410/1000], Training Loss: 0.4575, Test Loss: 0.4602, Best Test Loss: 0.4588\n",
      "Epoch [411/1000], Training Loss: 0.4582, Test Loss: 0.4585, Best Test Loss: 0.4585\n",
      "Epoch [412/1000], Training Loss: 0.4559, Test Loss: 0.4582, Best Test Loss: 0.4582\n",
      "Epoch [413/1000], Training Loss: 0.4580, Test Loss: 0.4582, Best Test Loss: 0.4582\n",
      "Epoch [414/1000], Training Loss: 0.4574, Test Loss: 0.4584, Best Test Loss: 0.4582\n",
      "Epoch [415/1000], Training Loss: 0.4554, Test Loss: 0.4577, Best Test Loss: 0.4577\n",
      "Epoch [416/1000], Training Loss: 0.4585, Test Loss: 0.4581, Best Test Loss: 0.4577\n",
      "Epoch [417/1000], Training Loss: 0.4565, Test Loss: 0.4587, Best Test Loss: 0.4577\n",
      "Epoch [418/1000], Training Loss: 0.4552, Test Loss: 0.4580, Best Test Loss: 0.4577\n",
      "Epoch [419/1000], Training Loss: 0.4555, Test Loss: 0.4590, Best Test Loss: 0.4577\n",
      "Epoch [420/1000], Training Loss: 0.4544, Test Loss: 0.4577, Best Test Loss: 0.4577\n",
      "Epoch [421/1000], Training Loss: 0.4579, Test Loss: 0.4577, Best Test Loss: 0.4577\n",
      "Epoch [422/1000], Training Loss: 0.4543, Test Loss: 0.4587, Best Test Loss: 0.4577\n",
      "Epoch [423/1000], Training Loss: 0.4564, Test Loss: 0.4575, Best Test Loss: 0.4575\n",
      "Epoch [424/1000], Training Loss: 0.4570, Test Loss: 0.4581, Best Test Loss: 0.4575\n",
      "Epoch [425/1000], Training Loss: 0.4558, Test Loss: 0.4579, Best Test Loss: 0.4575\n",
      "Epoch [426/1000], Training Loss: 0.4550, Test Loss: 0.4574, Best Test Loss: 0.4574\n",
      "Epoch [427/1000], Training Loss: 0.4538, Test Loss: 0.4600, Best Test Loss: 0.4574\n",
      "Epoch [428/1000], Training Loss: 0.4565, Test Loss: 0.4574, Best Test Loss: 0.4574\n",
      "Epoch [429/1000], Training Loss: 0.4566, Test Loss: 0.4587, Best Test Loss: 0.4574\n",
      "Epoch [430/1000], Training Loss: 0.4541, Test Loss: 0.4592, Best Test Loss: 0.4574\n",
      "Epoch [431/1000], Training Loss: 0.4526, Test Loss: 0.4572, Best Test Loss: 0.4572\n",
      "Epoch [432/1000], Training Loss: 0.4546, Test Loss: 0.4580, Best Test Loss: 0.4572\n",
      "Epoch [433/1000], Training Loss: 0.4518, Test Loss: 0.4581, Best Test Loss: 0.4572\n",
      "Epoch [434/1000], Training Loss: 0.4548, Test Loss: 0.4593, Best Test Loss: 0.4572\n",
      "Epoch [435/1000], Training Loss: 0.4548, Test Loss: 0.4572, Best Test Loss: 0.4572\n",
      "Epoch [436/1000], Training Loss: 0.4541, Test Loss: 0.4578, Best Test Loss: 0.4572\n",
      "Epoch [437/1000], Training Loss: 0.4531, Test Loss: 0.4566, Best Test Loss: 0.4566\n",
      "Epoch [438/1000], Training Loss: 0.4556, Test Loss: 0.4569, Best Test Loss: 0.4566\n",
      "Epoch [439/1000], Training Loss: 0.4536, Test Loss: 0.4577, Best Test Loss: 0.4566\n",
      "Epoch [440/1000], Training Loss: 0.4530, Test Loss: 0.4601, Best Test Loss: 0.4566\n",
      "Epoch [441/1000], Training Loss: 0.4543, Test Loss: 0.4576, Best Test Loss: 0.4566\n",
      "Epoch [442/1000], Training Loss: 0.4538, Test Loss: 0.4568, Best Test Loss: 0.4566\n",
      "Epoch [443/1000], Training Loss: 0.4540, Test Loss: 0.4562, Best Test Loss: 0.4562\n",
      "Epoch [444/1000], Training Loss: 0.4523, Test Loss: 0.4556, Best Test Loss: 0.4556\n",
      "Epoch [445/1000], Training Loss: 0.4527, Test Loss: 0.4568, Best Test Loss: 0.4556\n",
      "Epoch [446/1000], Training Loss: 0.4540, Test Loss: 0.4580, Best Test Loss: 0.4556\n",
      "Epoch [447/1000], Training Loss: 0.4531, Test Loss: 0.4577, Best Test Loss: 0.4556\n",
      "Epoch [448/1000], Training Loss: 0.4527, Test Loss: 0.4559, Best Test Loss: 0.4556\n",
      "Epoch [449/1000], Training Loss: 0.4494, Test Loss: 0.4567, Best Test Loss: 0.4556\n",
      "Epoch [450/1000], Training Loss: 0.4511, Test Loss: 0.4557, Best Test Loss: 0.4556\n",
      "Epoch [451/1000], Training Loss: 0.4504, Test Loss: 0.4572, Best Test Loss: 0.4556\n",
      "Epoch [452/1000], Training Loss: 0.4511, Test Loss: 0.4558, Best Test Loss: 0.4556\n",
      "Epoch [453/1000], Training Loss: 0.4531, Test Loss: 0.4565, Best Test Loss: 0.4556\n",
      "Epoch [454/1000], Training Loss: 0.4549, Test Loss: 0.4587, Best Test Loss: 0.4556\n",
      "Epoch [455/1000], Training Loss: 0.4510, Test Loss: 0.4561, Best Test Loss: 0.4556\n",
      "Epoch [456/1000], Training Loss: 0.4500, Test Loss: 0.4554, Best Test Loss: 0.4554\n",
      "Epoch [457/1000], Training Loss: 0.4559, Test Loss: 0.4558, Best Test Loss: 0.4554\n",
      "Epoch [458/1000], Training Loss: 0.4504, Test Loss: 0.4558, Best Test Loss: 0.4554\n",
      "Epoch [459/1000], Training Loss: 0.4492, Test Loss: 0.4558, Best Test Loss: 0.4554\n",
      "Epoch [460/1000], Training Loss: 0.4534, Test Loss: 0.4557, Best Test Loss: 0.4554\n",
      "Epoch [461/1000], Training Loss: 0.4514, Test Loss: 0.4553, Best Test Loss: 0.4553\n",
      "Epoch [462/1000], Training Loss: 0.4498, Test Loss: 0.4551, Best Test Loss: 0.4551\n",
      "Epoch [463/1000], Training Loss: 0.4490, Test Loss: 0.4570, Best Test Loss: 0.4551\n",
      "Epoch [464/1000], Training Loss: 0.4504, Test Loss: 0.4545, Best Test Loss: 0.4545\n",
      "Epoch [465/1000], Training Loss: 0.4521, Test Loss: 0.4548, Best Test Loss: 0.4545\n",
      "Epoch [466/1000], Training Loss: 0.4502, Test Loss: 0.4547, Best Test Loss: 0.4545\n",
      "Epoch [467/1000], Training Loss: 0.4508, Test Loss: 0.4555, Best Test Loss: 0.4545\n",
      "Epoch [468/1000], Training Loss: 0.4497, Test Loss: 0.4552, Best Test Loss: 0.4545\n",
      "Epoch [469/1000], Training Loss: 0.4495, Test Loss: 0.4559, Best Test Loss: 0.4545\n",
      "Epoch [470/1000], Training Loss: 0.4483, Test Loss: 0.4551, Best Test Loss: 0.4545\n",
      "Epoch [471/1000], Training Loss: 0.4509, Test Loss: 0.4546, Best Test Loss: 0.4545\n",
      "Epoch [472/1000], Training Loss: 0.4511, Test Loss: 0.4566, Best Test Loss: 0.4545\n",
      "Epoch [473/1000], Training Loss: 0.4497, Test Loss: 0.4579, Best Test Loss: 0.4545\n",
      "Epoch [474/1000], Training Loss: 0.4453, Test Loss: 0.4555, Best Test Loss: 0.4545\n",
      "Epoch [475/1000], Training Loss: 0.4478, Test Loss: 0.4548, Best Test Loss: 0.4545\n",
      "Epoch [476/1000], Training Loss: 0.4481, Test Loss: 0.4547, Best Test Loss: 0.4545\n",
      "Epoch [477/1000], Training Loss: 0.4473, Test Loss: 0.4549, Best Test Loss: 0.4545\n",
      "Epoch [478/1000], Training Loss: 0.4480, Test Loss: 0.4547, Best Test Loss: 0.4545\n",
      "Epoch [479/1000], Training Loss: 0.4494, Test Loss: 0.4559, Best Test Loss: 0.4545\n",
      "Epoch [480/1000], Training Loss: 0.4506, Test Loss: 0.4557, Best Test Loss: 0.4545\n",
      "Epoch [481/1000], Training Loss: 0.4470, Test Loss: 0.4553, Best Test Loss: 0.4545\n",
      "Epoch [482/1000], Training Loss: 0.4461, Test Loss: 0.4546, Best Test Loss: 0.4545\n",
      "Epoch [483/1000], Training Loss: 0.4474, Test Loss: 0.4554, Best Test Loss: 0.4545\n",
      "Epoch [484/1000], Training Loss: 0.4480, Test Loss: 0.4546, Best Test Loss: 0.4545\n",
      "Epoch [485/1000], Training Loss: 0.4446, Test Loss: 0.4545, Best Test Loss: 0.4545\n",
      "Epoch [486/1000], Training Loss: 0.4468, Test Loss: 0.4531, Best Test Loss: 0.4531\n",
      "Epoch [487/1000], Training Loss: 0.4487, Test Loss: 0.4537, Best Test Loss: 0.4531\n",
      "Epoch [488/1000], Training Loss: 0.4486, Test Loss: 0.4542, Best Test Loss: 0.4531\n",
      "Epoch [489/1000], Training Loss: 0.4484, Test Loss: 0.4559, Best Test Loss: 0.4531\n",
      "Epoch [490/1000], Training Loss: 0.4476, Test Loss: 0.4535, Best Test Loss: 0.4531\n",
      "Epoch [491/1000], Training Loss: 0.4476, Test Loss: 0.4540, Best Test Loss: 0.4531\n",
      "Epoch [492/1000], Training Loss: 0.4450, Test Loss: 0.4550, Best Test Loss: 0.4531\n",
      "Epoch [493/1000], Training Loss: 0.4463, Test Loss: 0.4546, Best Test Loss: 0.4531\n",
      "Epoch [494/1000], Training Loss: 0.4461, Test Loss: 0.4542, Best Test Loss: 0.4531\n",
      "Epoch [495/1000], Training Loss: 0.4461, Test Loss: 0.4546, Best Test Loss: 0.4531\n",
      "Epoch [496/1000], Training Loss: 0.4458, Test Loss: 0.4556, Best Test Loss: 0.4531\n",
      "Epoch [497/1000], Training Loss: 0.4458, Test Loss: 0.4553, Best Test Loss: 0.4531\n",
      "Epoch [498/1000], Training Loss: 0.4470, Test Loss: 0.4533, Best Test Loss: 0.4531\n",
      "Epoch [499/1000], Training Loss: 0.4462, Test Loss: 0.4540, Best Test Loss: 0.4531\n",
      "Epoch [500/1000], Training Loss: 0.4452, Test Loss: 0.4540, Best Test Loss: 0.4531\n",
      "Epoch [501/1000], Training Loss: 0.4431, Test Loss: 0.4547, Best Test Loss: 0.4531\n",
      "Epoch [502/1000], Training Loss: 0.4454, Test Loss: 0.4547, Best Test Loss: 0.4531\n",
      "Epoch [503/1000], Training Loss: 0.4461, Test Loss: 0.4529, Best Test Loss: 0.4529\n",
      "Epoch [504/1000], Training Loss: 0.4415, Test Loss: 0.4528, Best Test Loss: 0.4528\n",
      "Epoch [505/1000], Training Loss: 0.4477, Test Loss: 0.4529, Best Test Loss: 0.4528\n",
      "Epoch [506/1000], Training Loss: 0.4450, Test Loss: 0.4529, Best Test Loss: 0.4528\n",
      "Epoch [507/1000], Training Loss: 0.4454, Test Loss: 0.4528, Best Test Loss: 0.4528\n",
      "Epoch [508/1000], Training Loss: 0.4441, Test Loss: 0.4524, Best Test Loss: 0.4524\n",
      "Epoch [509/1000], Training Loss: 0.4463, Test Loss: 0.4532, Best Test Loss: 0.4524\n",
      "Epoch [510/1000], Training Loss: 0.4433, Test Loss: 0.4540, Best Test Loss: 0.4524\n",
      "Epoch [511/1000], Training Loss: 0.4446, Test Loss: 0.4532, Best Test Loss: 0.4524\n",
      "Epoch [512/1000], Training Loss: 0.4455, Test Loss: 0.4521, Best Test Loss: 0.4521\n",
      "Epoch [513/1000], Training Loss: 0.4474, Test Loss: 0.4525, Best Test Loss: 0.4521\n",
      "Epoch [514/1000], Training Loss: 0.4440, Test Loss: 0.4541, Best Test Loss: 0.4521\n",
      "Epoch [515/1000], Training Loss: 0.4458, Test Loss: 0.4539, Best Test Loss: 0.4521\n",
      "Epoch [516/1000], Training Loss: 0.4423, Test Loss: 0.4538, Best Test Loss: 0.4521\n",
      "Epoch [517/1000], Training Loss: 0.4455, Test Loss: 0.4534, Best Test Loss: 0.4521\n",
      "Epoch [518/1000], Training Loss: 0.4451, Test Loss: 0.4532, Best Test Loss: 0.4521\n",
      "Epoch [519/1000], Training Loss: 0.4451, Test Loss: 0.4531, Best Test Loss: 0.4521\n",
      "Epoch [520/1000], Training Loss: 0.4427, Test Loss: 0.4528, Best Test Loss: 0.4521\n",
      "Epoch [521/1000], Training Loss: 0.4452, Test Loss: 0.4530, Best Test Loss: 0.4521\n",
      "Epoch [522/1000], Training Loss: 0.4420, Test Loss: 0.4532, Best Test Loss: 0.4521\n",
      "Epoch [523/1000], Training Loss: 0.4438, Test Loss: 0.4533, Best Test Loss: 0.4521\n",
      "Epoch [524/1000], Training Loss: 0.4423, Test Loss: 0.4532, Best Test Loss: 0.4521\n",
      "Epoch [525/1000], Training Loss: 0.4405, Test Loss: 0.4524, Best Test Loss: 0.4521\n",
      "Epoch [526/1000], Training Loss: 0.4394, Test Loss: 0.4520, Best Test Loss: 0.4520\n",
      "Epoch [527/1000], Training Loss: 0.4402, Test Loss: 0.4516, Best Test Loss: 0.4516\n",
      "Epoch [528/1000], Training Loss: 0.4418, Test Loss: 0.4524, Best Test Loss: 0.4516\n",
      "Epoch [529/1000], Training Loss: 0.4435, Test Loss: 0.4508, Best Test Loss: 0.4508\n",
      "Epoch [530/1000], Training Loss: 0.4385, Test Loss: 0.4540, Best Test Loss: 0.4508\n",
      "Epoch [531/1000], Training Loss: 0.4414, Test Loss: 0.4520, Best Test Loss: 0.4508\n",
      "Epoch [532/1000], Training Loss: 0.4420, Test Loss: 0.4516, Best Test Loss: 0.4508\n",
      "Epoch [533/1000], Training Loss: 0.4414, Test Loss: 0.4519, Best Test Loss: 0.4508\n",
      "Epoch [534/1000], Training Loss: 0.4428, Test Loss: 0.4511, Best Test Loss: 0.4508\n",
      "Epoch [535/1000], Training Loss: 0.4424, Test Loss: 0.4509, Best Test Loss: 0.4508\n",
      "Epoch [536/1000], Training Loss: 0.4404, Test Loss: 0.4528, Best Test Loss: 0.4508\n",
      "Epoch [537/1000], Training Loss: 0.4412, Test Loss: 0.4516, Best Test Loss: 0.4508\n",
      "Epoch [538/1000], Training Loss: 0.4401, Test Loss: 0.4518, Best Test Loss: 0.4508\n",
      "Epoch [539/1000], Training Loss: 0.4423, Test Loss: 0.4508, Best Test Loss: 0.4508\n",
      "Epoch [540/1000], Training Loss: 0.4434, Test Loss: 0.4514, Best Test Loss: 0.4508\n",
      "Epoch [541/1000], Training Loss: 0.4398, Test Loss: 0.4515, Best Test Loss: 0.4508\n",
      "Epoch [542/1000], Training Loss: 0.4401, Test Loss: 0.4512, Best Test Loss: 0.4508\n",
      "Epoch [543/1000], Training Loss: 0.4419, Test Loss: 0.4516, Best Test Loss: 0.4508\n",
      "Epoch [544/1000], Training Loss: 0.4404, Test Loss: 0.4509, Best Test Loss: 0.4508\n",
      "Epoch [545/1000], Training Loss: 0.4421, Test Loss: 0.4529, Best Test Loss: 0.4508\n",
      "Epoch [546/1000], Training Loss: 0.4410, Test Loss: 0.4521, Best Test Loss: 0.4508\n",
      "Epoch [547/1000], Training Loss: 0.4382, Test Loss: 0.4508, Best Test Loss: 0.4508\n",
      "Epoch [548/1000], Training Loss: 0.4385, Test Loss: 0.4500, Best Test Loss: 0.4500\n",
      "Epoch [549/1000], Training Loss: 0.4371, Test Loss: 0.4506, Best Test Loss: 0.4500\n",
      "Epoch [550/1000], Training Loss: 0.4392, Test Loss: 0.4491, Best Test Loss: 0.4491\n",
      "Epoch [551/1000], Training Loss: 0.4432, Test Loss: 0.4516, Best Test Loss: 0.4491\n",
      "Epoch [552/1000], Training Loss: 0.4368, Test Loss: 0.4507, Best Test Loss: 0.4491\n",
      "Epoch [553/1000], Training Loss: 0.4384, Test Loss: 0.4500, Best Test Loss: 0.4491\n",
      "Epoch [554/1000], Training Loss: 0.4402, Test Loss: 0.4514, Best Test Loss: 0.4491\n",
      "Epoch [555/1000], Training Loss: 0.4412, Test Loss: 0.4494, Best Test Loss: 0.4491\n",
      "Epoch [556/1000], Training Loss: 0.4362, Test Loss: 0.4504, Best Test Loss: 0.4491\n",
      "Epoch [557/1000], Training Loss: 0.4415, Test Loss: 0.4511, Best Test Loss: 0.4491\n",
      "Epoch [558/1000], Training Loss: 0.4395, Test Loss: 0.4529, Best Test Loss: 0.4491\n",
      "Epoch [559/1000], Training Loss: 0.4376, Test Loss: 0.4498, Best Test Loss: 0.4491\n",
      "Epoch [560/1000], Training Loss: 0.4367, Test Loss: 0.4520, Best Test Loss: 0.4491\n",
      "Epoch [561/1000], Training Loss: 0.4384, Test Loss: 0.4525, Best Test Loss: 0.4491\n",
      "Epoch [562/1000], Training Loss: 0.4374, Test Loss: 0.4488, Best Test Loss: 0.4488\n",
      "Epoch [563/1000], Training Loss: 0.4387, Test Loss: 0.4525, Best Test Loss: 0.4488\n",
      "Epoch [564/1000], Training Loss: 0.4397, Test Loss: 0.4517, Best Test Loss: 0.4488\n",
      "Epoch [565/1000], Training Loss: 0.4366, Test Loss: 0.4497, Best Test Loss: 0.4488\n",
      "Epoch [566/1000], Training Loss: 0.4383, Test Loss: 0.4515, Best Test Loss: 0.4488\n",
      "Epoch [567/1000], Training Loss: 0.4373, Test Loss: 0.4515, Best Test Loss: 0.4488\n",
      "Epoch [568/1000], Training Loss: 0.4393, Test Loss: 0.4510, Best Test Loss: 0.4488\n",
      "Epoch [569/1000], Training Loss: 0.4394, Test Loss: 0.4491, Best Test Loss: 0.4488\n",
      "Epoch [570/1000], Training Loss: 0.4380, Test Loss: 0.4489, Best Test Loss: 0.4488\n",
      "Epoch [571/1000], Training Loss: 0.4382, Test Loss: 0.4487, Best Test Loss: 0.4487\n",
      "Epoch [572/1000], Training Loss: 0.4359, Test Loss: 0.4484, Best Test Loss: 0.4484\n",
      "Epoch [573/1000], Training Loss: 0.4353, Test Loss: 0.4491, Best Test Loss: 0.4484\n",
      "Epoch [574/1000], Training Loss: 0.4368, Test Loss: 0.4501, Best Test Loss: 0.4484\n",
      "Epoch [575/1000], Training Loss: 0.4367, Test Loss: 0.4493, Best Test Loss: 0.4484\n",
      "Epoch [576/1000], Training Loss: 0.4388, Test Loss: 0.4502, Best Test Loss: 0.4484\n",
      "Epoch [577/1000], Training Loss: 0.4353, Test Loss: 0.4497, Best Test Loss: 0.4484\n",
      "Epoch [578/1000], Training Loss: 0.4365, Test Loss: 0.4501, Best Test Loss: 0.4484\n",
      "Epoch [579/1000], Training Loss: 0.4343, Test Loss: 0.4490, Best Test Loss: 0.4484\n",
      "Epoch [580/1000], Training Loss: 0.4350, Test Loss: 0.4492, Best Test Loss: 0.4484\n",
      "Epoch [581/1000], Training Loss: 0.4348, Test Loss: 0.4498, Best Test Loss: 0.4484\n",
      "Epoch [582/1000], Training Loss: 0.4355, Test Loss: 0.4486, Best Test Loss: 0.4484\n",
      "Epoch [583/1000], Training Loss: 0.4355, Test Loss: 0.4490, Best Test Loss: 0.4484\n",
      "Epoch [584/1000], Training Loss: 0.4375, Test Loss: 0.4486, Best Test Loss: 0.4484\n",
      "Epoch [585/1000], Training Loss: 0.4366, Test Loss: 0.4484, Best Test Loss: 0.4484\n",
      "Epoch [586/1000], Training Loss: 0.4356, Test Loss: 0.4491, Best Test Loss: 0.4484\n",
      "Epoch [587/1000], Training Loss: 0.4370, Test Loss: 0.4494, Best Test Loss: 0.4484\n",
      "Epoch [588/1000], Training Loss: 0.4328, Test Loss: 0.4487, Best Test Loss: 0.4484\n",
      "Epoch [589/1000], Training Loss: 0.4356, Test Loss: 0.4510, Best Test Loss: 0.4484\n",
      "Epoch [590/1000], Training Loss: 0.4317, Test Loss: 0.4502, Best Test Loss: 0.4484\n",
      "Epoch [591/1000], Training Loss: 0.4369, Test Loss: 0.4490, Best Test Loss: 0.4484\n",
      "Epoch [592/1000], Training Loss: 0.4326, Test Loss: 0.4492, Best Test Loss: 0.4484\n",
      "Epoch [593/1000], Training Loss: 0.4346, Test Loss: 0.4490, Best Test Loss: 0.4484\n",
      "Epoch [594/1000], Training Loss: 0.4313, Test Loss: 0.4499, Best Test Loss: 0.4484\n",
      "Epoch [595/1000], Training Loss: 0.4362, Test Loss: 0.4488, Best Test Loss: 0.4484\n",
      "Epoch [596/1000], Training Loss: 0.4356, Test Loss: 0.4498, Best Test Loss: 0.4484\n",
      "Epoch [597/1000], Training Loss: 0.4326, Test Loss: 0.4496, Best Test Loss: 0.4484\n",
      "Epoch [598/1000], Training Loss: 0.4328, Test Loss: 0.4477, Best Test Loss: 0.4477\n",
      "Epoch [599/1000], Training Loss: 0.4332, Test Loss: 0.4497, Best Test Loss: 0.4477\n",
      "Epoch [600/1000], Training Loss: 0.4329, Test Loss: 0.4479, Best Test Loss: 0.4477\n",
      "Epoch [601/1000], Training Loss: 0.4337, Test Loss: 0.4493, Best Test Loss: 0.4477\n",
      "Epoch [602/1000], Training Loss: 0.4332, Test Loss: 0.4493, Best Test Loss: 0.4477\n",
      "Epoch [603/1000], Training Loss: 0.4332, Test Loss: 0.4481, Best Test Loss: 0.4477\n",
      "Epoch [604/1000], Training Loss: 0.4369, Test Loss: 0.4477, Best Test Loss: 0.4477\n",
      "Epoch [605/1000], Training Loss: 0.4320, Test Loss: 0.4484, Best Test Loss: 0.4477\n",
      "Epoch [606/1000], Training Loss: 0.4340, Test Loss: 0.4486, Best Test Loss: 0.4477\n",
      "Epoch [607/1000], Training Loss: 0.4328, Test Loss: 0.4481, Best Test Loss: 0.4477\n",
      "Epoch [608/1000], Training Loss: 0.4319, Test Loss: 0.4482, Best Test Loss: 0.4477\n",
      "Epoch [609/1000], Training Loss: 0.4338, Test Loss: 0.4488, Best Test Loss: 0.4477\n",
      "Epoch [610/1000], Training Loss: 0.4349, Test Loss: 0.4469, Best Test Loss: 0.4469\n",
      "Epoch [611/1000], Training Loss: 0.4335, Test Loss: 0.4473, Best Test Loss: 0.4469\n",
      "Epoch [612/1000], Training Loss: 0.4320, Test Loss: 0.4481, Best Test Loss: 0.4469\n",
      "Epoch [613/1000], Training Loss: 0.4308, Test Loss: 0.4489, Best Test Loss: 0.4469\n",
      "Epoch [614/1000], Training Loss: 0.4306, Test Loss: 0.4483, Best Test Loss: 0.4469\n",
      "Epoch [615/1000], Training Loss: 0.4320, Test Loss: 0.4477, Best Test Loss: 0.4469\n",
      "Epoch [616/1000], Training Loss: 0.4335, Test Loss: 0.4485, Best Test Loss: 0.4469\n",
      "Epoch [617/1000], Training Loss: 0.4350, Test Loss: 0.4490, Best Test Loss: 0.4469\n",
      "Epoch [618/1000], Training Loss: 0.4315, Test Loss: 0.4495, Best Test Loss: 0.4469\n",
      "Epoch [619/1000], Training Loss: 0.4336, Test Loss: 0.4493, Best Test Loss: 0.4469\n",
      "Epoch [620/1000], Training Loss: 0.4325, Test Loss: 0.4483, Best Test Loss: 0.4469\n",
      "Epoch [621/1000], Training Loss: 0.4310, Test Loss: 0.4474, Best Test Loss: 0.4469\n",
      "Epoch [622/1000], Training Loss: 0.4315, Test Loss: 0.4473, Best Test Loss: 0.4469\n",
      "Epoch [623/1000], Training Loss: 0.4348, Test Loss: 0.4474, Best Test Loss: 0.4469\n",
      "Epoch [624/1000], Training Loss: 0.4326, Test Loss: 0.4469, Best Test Loss: 0.4469\n",
      "Epoch [625/1000], Training Loss: 0.4321, Test Loss: 0.4478, Best Test Loss: 0.4469\n",
      "Epoch [626/1000], Training Loss: 0.4303, Test Loss: 0.4469, Best Test Loss: 0.4469\n",
      "Epoch [627/1000], Training Loss: 0.4333, Test Loss: 0.4474, Best Test Loss: 0.4469\n",
      "Epoch [628/1000], Training Loss: 0.4298, Test Loss: 0.4496, Best Test Loss: 0.4469\n",
      "Epoch [629/1000], Training Loss: 0.4325, Test Loss: 0.4508, Best Test Loss: 0.4469\n",
      "Epoch [630/1000], Training Loss: 0.4294, Test Loss: 0.4496, Best Test Loss: 0.4469\n",
      "Epoch [631/1000], Training Loss: 0.4294, Test Loss: 0.4475, Best Test Loss: 0.4469\n",
      "Epoch [632/1000], Training Loss: 0.4319, Test Loss: 0.4477, Best Test Loss: 0.4469\n",
      "Epoch [633/1000], Training Loss: 0.4292, Test Loss: 0.4465, Best Test Loss: 0.4465\n",
      "Epoch [634/1000], Training Loss: 0.4300, Test Loss: 0.4480, Best Test Loss: 0.4465\n",
      "Epoch [635/1000], Training Loss: 0.4323, Test Loss: 0.4495, Best Test Loss: 0.4465\n",
      "Epoch [636/1000], Training Loss: 0.4305, Test Loss: 0.4474, Best Test Loss: 0.4465\n",
      "Epoch [637/1000], Training Loss: 0.4322, Test Loss: 0.4480, Best Test Loss: 0.4465\n",
      "Epoch [638/1000], Training Loss: 0.4300, Test Loss: 0.4482, Best Test Loss: 0.4465\n",
      "Epoch [639/1000], Training Loss: 0.4278, Test Loss: 0.4470, Best Test Loss: 0.4465\n",
      "Epoch [640/1000], Training Loss: 0.4292, Test Loss: 0.4462, Best Test Loss: 0.4462\n",
      "Epoch [641/1000], Training Loss: 0.4284, Test Loss: 0.4465, Best Test Loss: 0.4462\n",
      "Epoch [642/1000], Training Loss: 0.4281, Test Loss: 0.4492, Best Test Loss: 0.4462\n",
      "Epoch [643/1000], Training Loss: 0.4304, Test Loss: 0.4466, Best Test Loss: 0.4462\n",
      "Epoch [644/1000], Training Loss: 0.4279, Test Loss: 0.4465, Best Test Loss: 0.4462\n",
      "Epoch [645/1000], Training Loss: 0.4311, Test Loss: 0.4467, Best Test Loss: 0.4462\n",
      "Epoch [646/1000], Training Loss: 0.4320, Test Loss: 0.4466, Best Test Loss: 0.4462\n",
      "Epoch [647/1000], Training Loss: 0.4295, Test Loss: 0.4467, Best Test Loss: 0.4462\n",
      "Epoch [648/1000], Training Loss: 0.4300, Test Loss: 0.4474, Best Test Loss: 0.4462\n",
      "Epoch [649/1000], Training Loss: 0.4301, Test Loss: 0.4467, Best Test Loss: 0.4462\n",
      "Epoch [650/1000], Training Loss: 0.4279, Test Loss: 0.4466, Best Test Loss: 0.4462\n",
      "Epoch [651/1000], Training Loss: 0.4281, Test Loss: 0.4460, Best Test Loss: 0.4460\n",
      "Epoch [652/1000], Training Loss: 0.4298, Test Loss: 0.4474, Best Test Loss: 0.4460\n",
      "Epoch [653/1000], Training Loss: 0.4284, Test Loss: 0.4460, Best Test Loss: 0.4460\n",
      "Epoch [654/1000], Training Loss: 0.4267, Test Loss: 0.4455, Best Test Loss: 0.4455\n",
      "Epoch [655/1000], Training Loss: 0.4297, Test Loss: 0.4469, Best Test Loss: 0.4455\n",
      "Epoch [656/1000], Training Loss: 0.4300, Test Loss: 0.4459, Best Test Loss: 0.4455\n",
      "Epoch [657/1000], Training Loss: 0.4255, Test Loss: 0.4460, Best Test Loss: 0.4455\n",
      "Epoch [658/1000], Training Loss: 0.4282, Test Loss: 0.4456, Best Test Loss: 0.4455\n",
      "Epoch [659/1000], Training Loss: 0.4260, Test Loss: 0.4452, Best Test Loss: 0.4452\n",
      "Epoch [660/1000], Training Loss: 0.4283, Test Loss: 0.4449, Best Test Loss: 0.4449\n",
      "Epoch [661/1000], Training Loss: 0.4260, Test Loss: 0.4461, Best Test Loss: 0.4449\n",
      "Epoch [662/1000], Training Loss: 0.4298, Test Loss: 0.4473, Best Test Loss: 0.4449\n",
      "Epoch [663/1000], Training Loss: 0.4279, Test Loss: 0.4448, Best Test Loss: 0.4448\n",
      "Epoch [664/1000], Training Loss: 0.4269, Test Loss: 0.4454, Best Test Loss: 0.4448\n",
      "Epoch [665/1000], Training Loss: 0.4269, Test Loss: 0.4470, Best Test Loss: 0.4448\n",
      "Epoch [666/1000], Training Loss: 0.4261, Test Loss: 0.4479, Best Test Loss: 0.4448\n",
      "Epoch [667/1000], Training Loss: 0.4283, Test Loss: 0.4474, Best Test Loss: 0.4448\n",
      "Epoch [668/1000], Training Loss: 0.4252, Test Loss: 0.4461, Best Test Loss: 0.4448\n",
      "Epoch [669/1000], Training Loss: 0.4257, Test Loss: 0.4453, Best Test Loss: 0.4448\n",
      "Epoch [670/1000], Training Loss: 0.4262, Test Loss: 0.4463, Best Test Loss: 0.4448\n",
      "Epoch [671/1000], Training Loss: 0.4265, Test Loss: 0.4455, Best Test Loss: 0.4448\n",
      "Epoch [672/1000], Training Loss: 0.4263, Test Loss: 0.4449, Best Test Loss: 0.4448\n",
      "Epoch [673/1000], Training Loss: 0.4260, Test Loss: 0.4455, Best Test Loss: 0.4448\n",
      "Epoch [674/1000], Training Loss: 0.4298, Test Loss: 0.4460, Best Test Loss: 0.4448\n",
      "Epoch [675/1000], Training Loss: 0.4270, Test Loss: 0.4460, Best Test Loss: 0.4448\n",
      "Epoch [676/1000], Training Loss: 0.4266, Test Loss: 0.4451, Best Test Loss: 0.4448\n",
      "Epoch [677/1000], Training Loss: 0.4230, Test Loss: 0.4449, Best Test Loss: 0.4448\n",
      "Epoch [678/1000], Training Loss: 0.4264, Test Loss: 0.4454, Best Test Loss: 0.4448\n",
      "Epoch [679/1000], Training Loss: 0.4271, Test Loss: 0.4466, Best Test Loss: 0.4448\n",
      "Epoch [680/1000], Training Loss: 0.4266, Test Loss: 0.4468, Best Test Loss: 0.4448\n",
      "Epoch [681/1000], Training Loss: 0.4259, Test Loss: 0.4453, Best Test Loss: 0.4448\n",
      "Epoch [682/1000], Training Loss: 0.4226, Test Loss: 0.4445, Best Test Loss: 0.4445\n",
      "Epoch [683/1000], Training Loss: 0.4293, Test Loss: 0.4462, Best Test Loss: 0.4445\n",
      "Epoch [684/1000], Training Loss: 0.4267, Test Loss: 0.4463, Best Test Loss: 0.4445\n",
      "Epoch [685/1000], Training Loss: 0.4234, Test Loss: 0.4477, Best Test Loss: 0.4445\n",
      "Epoch [686/1000], Training Loss: 0.4263, Test Loss: 0.4460, Best Test Loss: 0.4445\n",
      "Epoch [687/1000], Training Loss: 0.4280, Test Loss: 0.4461, Best Test Loss: 0.4445\n",
      "Epoch [688/1000], Training Loss: 0.4259, Test Loss: 0.4447, Best Test Loss: 0.4445\n",
      "Epoch [689/1000], Training Loss: 0.4261, Test Loss: 0.4453, Best Test Loss: 0.4445\n",
      "Epoch [690/1000], Training Loss: 0.4259, Test Loss: 0.4465, Best Test Loss: 0.4445\n",
      "Epoch [691/1000], Training Loss: 0.4224, Test Loss: 0.4452, Best Test Loss: 0.4445\n",
      "Epoch [692/1000], Training Loss: 0.4242, Test Loss: 0.4454, Best Test Loss: 0.4445\n",
      "Epoch [693/1000], Training Loss: 0.4251, Test Loss: 0.4447, Best Test Loss: 0.4445\n",
      "Epoch [694/1000], Training Loss: 0.4214, Test Loss: 0.4459, Best Test Loss: 0.4445\n",
      "Epoch [695/1000], Training Loss: 0.4226, Test Loss: 0.4446, Best Test Loss: 0.4445\n",
      "Epoch [696/1000], Training Loss: 0.4215, Test Loss: 0.4453, Best Test Loss: 0.4445\n",
      "Epoch [697/1000], Training Loss: 0.4251, Test Loss: 0.4445, Best Test Loss: 0.4445\n",
      "Epoch [698/1000], Training Loss: 0.4230, Test Loss: 0.4448, Best Test Loss: 0.4445\n",
      "Epoch [699/1000], Training Loss: 0.4239, Test Loss: 0.4457, Best Test Loss: 0.4445\n",
      "Epoch [700/1000], Training Loss: 0.4223, Test Loss: 0.4454, Best Test Loss: 0.4445\n",
      "Epoch [701/1000], Training Loss: 0.4211, Test Loss: 0.4456, Best Test Loss: 0.4445\n",
      "Epoch [702/1000], Training Loss: 0.4203, Test Loss: 0.4451, Best Test Loss: 0.4445\n",
      "Epoch [703/1000], Training Loss: 0.4211, Test Loss: 0.4441, Best Test Loss: 0.4441\n",
      "Epoch [704/1000], Training Loss: 0.4222, Test Loss: 0.4452, Best Test Loss: 0.4441\n",
      "Epoch [705/1000], Training Loss: 0.4201, Test Loss: 0.4447, Best Test Loss: 0.4441\n",
      "Epoch [706/1000], Training Loss: 0.4212, Test Loss: 0.4445, Best Test Loss: 0.4441\n",
      "Epoch [707/1000], Training Loss: 0.4208, Test Loss: 0.4454, Best Test Loss: 0.4441\n",
      "Epoch [708/1000], Training Loss: 0.4234, Test Loss: 0.4456, Best Test Loss: 0.4441\n",
      "Epoch [709/1000], Training Loss: 0.4232, Test Loss: 0.4450, Best Test Loss: 0.4441\n",
      "Epoch [710/1000], Training Loss: 0.4222, Test Loss: 0.4438, Best Test Loss: 0.4438\n",
      "Epoch [711/1000], Training Loss: 0.4233, Test Loss: 0.4435, Best Test Loss: 0.4435\n",
      "Epoch [712/1000], Training Loss: 0.4202, Test Loss: 0.4437, Best Test Loss: 0.4435\n",
      "Epoch [713/1000], Training Loss: 0.4234, Test Loss: 0.4436, Best Test Loss: 0.4435\n",
      "Epoch [714/1000], Training Loss: 0.4215, Test Loss: 0.4441, Best Test Loss: 0.4435\n",
      "Epoch [715/1000], Training Loss: 0.4224, Test Loss: 0.4449, Best Test Loss: 0.4435\n",
      "Epoch [716/1000], Training Loss: 0.4235, Test Loss: 0.4464, Best Test Loss: 0.4435\n",
      "Epoch [717/1000], Training Loss: 0.4226, Test Loss: 0.4449, Best Test Loss: 0.4435\n",
      "Epoch [718/1000], Training Loss: 0.4236, Test Loss: 0.4447, Best Test Loss: 0.4435\n",
      "Epoch [719/1000], Training Loss: 0.4227, Test Loss: 0.4438, Best Test Loss: 0.4435\n",
      "Epoch [720/1000], Training Loss: 0.4216, Test Loss: 0.4455, Best Test Loss: 0.4435\n",
      "Epoch [721/1000], Training Loss: 0.4222, Test Loss: 0.4446, Best Test Loss: 0.4435\n",
      "Epoch [722/1000], Training Loss: 0.4257, Test Loss: 0.4456, Best Test Loss: 0.4435\n",
      "Epoch [723/1000], Training Loss: 0.4175, Test Loss: 0.4443, Best Test Loss: 0.4435\n",
      "Epoch [724/1000], Training Loss: 0.4196, Test Loss: 0.4432, Best Test Loss: 0.4432\n",
      "Epoch [725/1000], Training Loss: 0.4199, Test Loss: 0.4429, Best Test Loss: 0.4429\n",
      "Epoch [726/1000], Training Loss: 0.4228, Test Loss: 0.4439, Best Test Loss: 0.4429\n",
      "Epoch [727/1000], Training Loss: 0.4222, Test Loss: 0.4442, Best Test Loss: 0.4429\n",
      "Epoch [728/1000], Training Loss: 0.4208, Test Loss: 0.4437, Best Test Loss: 0.4429\n",
      "Epoch [729/1000], Training Loss: 0.4218, Test Loss: 0.4453, Best Test Loss: 0.4429\n",
      "Epoch [730/1000], Training Loss: 0.4215, Test Loss: 0.4467, Best Test Loss: 0.4429\n",
      "Epoch [731/1000], Training Loss: 0.4218, Test Loss: 0.4447, Best Test Loss: 0.4429\n",
      "Epoch [732/1000], Training Loss: 0.4204, Test Loss: 0.4440, Best Test Loss: 0.4429\n",
      "Epoch [733/1000], Training Loss: 0.4213, Test Loss: 0.4451, Best Test Loss: 0.4429\n",
      "Epoch [734/1000], Training Loss: 0.4227, Test Loss: 0.4457, Best Test Loss: 0.4429\n",
      "Epoch [735/1000], Training Loss: 0.4189, Test Loss: 0.4454, Best Test Loss: 0.4429\n",
      "Epoch [736/1000], Training Loss: 0.4178, Test Loss: 0.4433, Best Test Loss: 0.4429\n",
      "Epoch [737/1000], Training Loss: 0.4203, Test Loss: 0.4439, Best Test Loss: 0.4429\n",
      "Epoch [738/1000], Training Loss: 0.4190, Test Loss: 0.4446, Best Test Loss: 0.4429\n",
      "Epoch [739/1000], Training Loss: 0.4192, Test Loss: 0.4429, Best Test Loss: 0.4429\n",
      "Epoch [740/1000], Training Loss: 0.4189, Test Loss: 0.4435, Best Test Loss: 0.4429\n",
      "Epoch [741/1000], Training Loss: 0.4171, Test Loss: 0.4454, Best Test Loss: 0.4429\n",
      "Epoch [742/1000], Training Loss: 0.4209, Test Loss: 0.4435, Best Test Loss: 0.4429\n",
      "Epoch [743/1000], Training Loss: 0.4209, Test Loss: 0.4447, Best Test Loss: 0.4429\n",
      "Epoch [744/1000], Training Loss: 0.4176, Test Loss: 0.4433, Best Test Loss: 0.4429\n",
      "Epoch [745/1000], Training Loss: 0.4170, Test Loss: 0.4449, Best Test Loss: 0.4429\n",
      "Epoch [746/1000], Training Loss: 0.4167, Test Loss: 0.4478, Best Test Loss: 0.4429\n",
      "Epoch [747/1000], Training Loss: 0.4201, Test Loss: 0.4441, Best Test Loss: 0.4429\n",
      "Epoch [748/1000], Training Loss: 0.4183, Test Loss: 0.4437, Best Test Loss: 0.4429\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "early_stopper = EarlyStopper(patience=20, min_delta=0.003)\n",
    "best_test_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0   \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_test_loss += loss.item()\n",
    "\n",
    "    if (tl := running_test_loss / len(test_loader)) < best_test_loss:\n",
    "        best_test_loss = tl\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\n",
    "        f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "        f'Training Loss: {running_train_loss / len(train_loader):.4f}, '\n",
    "        f'Test Loss: {running_test_loss / len(test_loader):.4f}, '\n",
    "        f'Best Test Loss: {best_test_loss:.4f}'\n",
    "    )\n",
    "\n",
    "    if early_stopper.early_stop(running_test_loss):\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_97722/1833195287.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7840\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            predicted = (outputs >= 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
